# Follows https://arxiv.org/pdf/2209.12782#page=13.82
dataset:
  _target_: datasets.amp.AMPRewardProxyDataset
  split: 'D'
  nfold: 5

network:
  _target_: model.transformer.TransformerRewardModel # TODO:FIXME
  encoder_config:
    max_len: 60
    num_heads: 8
    num_layers: 4

    mlp_dim: 64
    qkv_dim: 64
    emb_dim: 64

    dropout_rate: 0.0
    vocab_size: 26

  output_hid: 512
  output_dims: 1

training:
  batch_size: 256
  learning_rate: 1e-4
  weight_decay: 1e-6
  num_epochs: 1000    # Very large number, but we will use early stopping
  val_each_epoch: 2   
  early_stop_tol: 5
  task: 'classification'

seed: 1
save_path: /home/labcmap/daniil.tiapkin/gfnx/proxy/weights/amp/